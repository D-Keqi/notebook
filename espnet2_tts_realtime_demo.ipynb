{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "espnet2_tts_demo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMSw_r1uRm4a"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/espnet/notebook/blob/master/espnet2_tts_realtime_demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuhqhYSToxl7"
      },
      "source": [
        "# ESPnet2-TTS realtime demonstration\n",
        "\n",
        "This notebook provides a demonstration of the realtime E2E-TTS using ESPnet2-TTS and ParallelWaveGAN (+ MelGAN).\n",
        "\n",
        "- ESPnet2-TTS: https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE/tts1\n",
        "- ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\n",
        "\n",
        "Author: Tomoki Hayashi ([@kan-bayashi](https://github.com/kan-bayashi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_i_gdgAFNJ"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjJ5zkyaoy29"
      },
      "source": [
        "# NOTE: pip shows imcompatible errors due to preinstalled libraries but you do not need to care\n",
        "!pip install -q espnet==0.9.7 parallel_wavegan==0.4.8\n",
        "!pip install -q espnet_model_zoo\n",
        "!pip install -q pyopenjtalk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYLn3bL-qQjN"
      },
      "source": [
        "## Single speaker model demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as4iFXid0m4f"
      },
      "source": [
        "### Model Selection\n",
        "\n",
        "Please select model: English, Japanese, and Mandarin are supported.\n",
        "\n",
        "You can try Tacotron2, FastSpeech, and FastSpeech2 as the text2mel model.  \n",
        "And you can use Parallel WaveGAN and Multi-band MelGAN as the vocoder model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Choose English model { run: \"auto\" }\n",
        "\n",
        "lang = 'English'\n",
        "fs = 22050 #@param {type:\"integer\"}\n",
        "tag = 'kan-bayashi/ljspeech_conformer_fastspeech2' #@param [\"kan-bayashi/ljspeech_tacotron2\", \"kan-bayashi/ljspeech_fastspeech\", \"kan-bayashi/ljspeech_fastspeech2\", \"kan-bayashi/ljspeech_conformer_fastspeech2\"] {type:\"string\"}\n",
        "vocoder_tag = \"ljspeech_parallel_wavegan.v1\" #@param [\"ljspeech_parallel_wavegan.v1\", \"ljspeech_full_band_melgan.v2\", \"ljspeech_multi_band_melgan.v2\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Choose Japanese model { run: \"auto\" }\n",
        "\n",
        "lang = 'Japanese'\n",
        "fs = 24000 #@param {type:\"integer\"}\n",
        "tag = 'kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause' #@param [\"kan-bayashi/jsut_tacotron2\", \"kan-bayashi/jsut_transformer\", \"kan-bayashi/jsut_fastspeech\", \"kan-bayashi/jsut_fastspeech2\", \"kan-bayashi/jsut_conformer_fastspeech2\", \"kan-bayashi/jsut_conformer_fastspeech2_accent\", \"kan-bayashi/jsut_conformer_fastspeech2_accent_with_pause\"] {type:\"string\"}\n",
        "vocoder_tag = \"jsut_parallel_wavegan.v1\" #@param [\"jsut_parallel_wavegan.v1\", \"jsut_multi_band_melgan.v2\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Choose Mandarin model { run: \"auto\" }\n",
        "\n",
        "lang = 'Mandarin'\n",
        "fs = 24000 #@param {type:\"integer\"}\n",
        "tag = 'kan-bayashi/csmsc_conformer_fastspeech2' #@param [\"kan-bayashi/csmsc_tacotron2\", \"kan-bayashi/csmsc_transformer\", \"kan-bayashi/csmsc_fastspeech\", \"kan-bayashi/csmsc_fastspeech2\", \"kan-bayashi/csmsc_conformer_fastspeech2\"] {type: \"string\"}\n",
        "vocoder_tag = \"csmsc_parallel_wavegan.v1\" #@param [\"csmsc_parallel_wavegan.v1\", \"csmsc_multi_band_melgan.v2\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9S-SFPe0z0w"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z64fD2UgjJ6Q"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "from espnet2.bin.tts_inference import Text2Speech\n",
        "from parallel_wavegan.utils import download_pretrained_model\n",
        "from parallel_wavegan.utils import load_model\n",
        "d = ModelDownloader()\n",
        "text2speech = Text2Speech(\n",
        "    **d.download_and_unpack(tag),\n",
        "    device=\"cuda\",\n",
        "    # Only for Tacotron 2\n",
        "    threshold=0.5,\n",
        "    minlenratio=0.0,\n",
        "    maxlenratio=10.0,\n",
        "    use_att_constraint=False,\n",
        "    backward_window=1,\n",
        "    forward_window=3,\n",
        "    # Only for FastSpeech & FastSpeech2\n",
        "    speed_control_alpha=1.0,\n",
        ")\n",
        "text2speech.spc2wav = None  # Disable griffin-lim\n",
        "# NOTE: Sometimes download is failed due to \"Permission denied\". That is \n",
        "#   the limitation of google drive. Please retry after serveral hours.\n",
        "vocoder = load_model(download_pretrained_model(vocoder_tag)).to(\"cuda\").eval()\n",
        "vocoder.remove_weight_norm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMaT0Zev021a"
      },
      "source": [
        "### Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrRM57hhgtHy"
      },
      "source": [
        "# decide the input sentence by yourself\n",
        "print(f\"Input your favorite sentence in {lang}.\")\n",
        "x = input()\n",
        "\n",
        "# synthesis\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    wav, c, *_ = text2speech(x)\n",
        "    wav = vocoder.inference(c)\n",
        "rtf = (time.time() - start) / (len(wav) / fs)\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "\n",
        "# let us listen to generated samples\n",
        "from IPython.display import display, Audio\n",
        "display(Audio(wav.view(-1).cpu().numpy(), rate=fs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TTAygALqY6T"
      },
      "source": [
        "## Multi-speaker Model Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSEZYh22n4gn"
      },
      "source": [
        "### Model Selection\n",
        "\n",
        "Please select models by comment out.\n",
        "\n",
        "Now we provide only English multi-speaker pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title English multi-speaker pretrained model { run: \"auto\" }\n",
        "\n",
        "lang = 'English'\n",
        "fs = 24000 #@param {type:\"integer\"}\n",
        "tag = 'kan-bayashi/libritts_gst+xvector_conformer_fastspeech2' #@param [\"kan-bayashi/vctk_gst_tacotron2\", \"kan-bayashi/vctk_gst_transformer\", \"kan-bayashi/vctk_xvector_tacotron2\", \"kan-bayashi/vctk_xvector_transformer\", \"kan-bayashi/vctk_xvector_conformer_fastspeech2\", \"kan-bayashi/vctk_gst+xvector_tacotron2\", \"kan-bayashi/vctk_gst+xvector_transformer\", \"kan-bayashi/vctk_gst+xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_xvector_transformer\", \"kan-bayashi/libritts_xvector_conformer_fastspeech2\", \"kan-bayashi/libritts_gst+xvector_transformer\", \"kan-bayashi/libritts_gst+xvector_conformer_fastspeech2\"] {type:\"string\"}\n",
        "vocoder_tag = \"libritts_parallel_wavegan.v1.long\" #@param [\"vctk_parallel_wavegan.v1.long\", \"vctk_multi_band_melgan.v2\", \"libritts_parallel_wavegan.v1.long\", \"libritts_multi_band_melgan.v2\"] {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcshmgYpoVzh"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfJFD4QroNhJ"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "from espnet_model_zoo.downloader import ModelDownloader\n",
        "from espnet2.bin.tts_inference import Text2Speech\n",
        "from parallel_wavegan.utils import download_pretrained_model\n",
        "from parallel_wavegan.utils import load_model\n",
        "d = ModelDownloader()\n",
        "text2speech = Text2Speech(\n",
        "    **d.download_and_unpack(tag),\n",
        "    device=\"cuda\",\n",
        "    # Only for Tacotron 2\n",
        "    threshold=0.5,\n",
        "    minlenratio=0.0,\n",
        "    maxlenratio=10.0,\n",
        "    use_att_constraint=False,\n",
        "    backward_window=1,\n",
        "    forward_window=3,\n",
        "    # Only for FastSpeech & FastSpeech2\n",
        "    speed_control_alpha=1.0,\n",
        ")\n",
        "text2speech.spc2wav = None  # Disable griffin-lim\n",
        "# NOTE: Sometimes download is failed due to \"Permission denied\". That is \n",
        "#   the limitation of google drive. Please retry after serveral hours.\n",
        "vocoder = load_model(download_pretrained_model(vocoder_tag)).to(\"cuda\").eval()\n",
        "vocoder.remove_weight_norm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdaMNwrtuZhY"
      },
      "source": [
        "### Speaker selection\n",
        "\n",
        "For multi-speaker model, we need to provide X-vector and/or the reference speech to decide the speaker characteristics.  \n",
        "For X-vector, you can select the speaker from the dumped x-vectors.  \n",
        "For the reference speech, you can use any speech but please make sure the sampling rate is matched."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzoAd1rgObcP"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import kaldiio\n",
        "\n",
        "# X-vector selection\n",
        "if text2speech.tts.spk_embed_dim is not None:\n",
        "    # load x-vector\n",
        "    model_dir = os.path.dirname(d.download_and_unpack(tag)[\"train_config\"])\n",
        "    xvector_ark = f\"{model_dir}/../../dump/xvector/tr_no_dev/spk_xvector.ark\"  # training speakers\n",
        "    # xvector_ark = f\"{model_dir}/../../dump/xvector/dev/spk_xvector.ark\"  # development speakers\n",
        "    # xvector_ark = f\"{model_dir}/../../dump/xvector/eval1/spk_xvector.ark\"  # eval speakers\n",
        "    xvectors = {k: v for k, v in kaldiio.load_ark(xvector_ark)}\n",
        "    spks = list(xvectors.keys())\n",
        "\n",
        "    # randomly select speaker\n",
        "    random_spk_idx = np.random.randint(0, len(spks))\n",
        "    spk = spks[random_spk_idx]\n",
        "    spembs = xvectors[spk]\n",
        "    print(f\"selected spk: {spk}\")\n",
        "\n",
        "# Reference speech selection for GST\n",
        "if text2speech.use_speech:\n",
        "    # you can change here to load your own reference speech\n",
        "    # e.g.\n",
        "    # import soundfile as sf\n",
        "    # speech, fs = sf.read(\"/path/to/reference.wav\")\n",
        "    # speech = torch.from_numpy(speech).float()\n",
        "    speech = torch.randn(50000,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6G-1YW9ocYV"
      },
      "source": [
        "### Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o87zK1NLobne"
      },
      "source": [
        "# decide the input sentence by yourself\n",
        "print(f\"Input your favorite sentence in {lang}.\")\n",
        "x = input()\n",
        "\n",
        "# synthesis\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    wav, c, *_ = text2speech(x, speech=speech, spembs=spembs)\n",
        "    wav = vocoder.inference(c)\n",
        "rtf = (time.time() - start) / (len(wav) / fs)\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "\n",
        "# let us listen to generated samples\n",
        "from IPython.display import display, Audio\n",
        "display(Audio(wav.view(-1).cpu().numpy(), rate=fs))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
